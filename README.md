# ollama_PDF_CHAT
PDF_Chat w/Ollama LLMs
Once setup, this system should be able to fun offline

This is a basic streamlit powered personal pdfchat webapp.
It is self hosted with ollama.

Install ollama
Pull the necessary models.
This uses llama2-uncensored for the chat, should work with all ollama models

Clone Repo, cd to clone path

Setup your eviroment & install modules: "pip install -r requirements.txt"

Run "ollama serve"

Run "streamlit run app.py"

![Screenshot from 2023-11-14 21-43-58](https://github.com/StuckNSpace/ollama_PDF_CHAT/assets/34696551/fb2b16ca-c738-4e71-bc24-2d3c1f3cbb4f)
![Screenshot from 2023-11-14 22-01-28](https://github.com/StuckNSpace/ollama_PDF_CHAT/assets/34696551/c0912f34-aab0-4f99-9df0-9ecf41f6e920)


Inspired by https://github.com/alejandro-ao/ask-multiple-pdfs
